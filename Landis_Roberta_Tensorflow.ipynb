{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of  NLP QA Kaggle distilbert Cases.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DianaMoyano1/NLP-Sentiment_Extraction_Challenge/blob/master/Landis_Roberta_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrGR6ljGnRWL",
        "colab_type": "text"
      },
      "source": [
        "# SECTION 1: Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SclTK2sEy2U",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#install the following first\n",
        "!pip install transformers==2.11.0 --quiet\n",
        "!pip install tensorflow==2.2.0 --quiet\n",
        "!pip install tensorboardX --quiet\n",
        "!pip install sklearn --quiet\n",
        "!pip install tokenizers --quiet\n",
        "!pip install numpy --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-f6h_O-ax7b",
        "colab_type": "text"
      },
      "source": [
        "### Setup NVIDIA APEX\n",
        "\n",
        "Tool to enable mixed precision training. More info here: https://github.com/NVIDIA/apex"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EKbT79HZ1FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile setup.sh\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ8BU4o8aAxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this will take 10mins to run\n",
        "import timeit\n",
        "start = timeit.default_timer()\n",
        "\n",
        "!sh setup.sh --quiet\n",
        "\n",
        "stop = timeit.default_timer()\n",
        "print('Time: ', stop - start)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sonPGlnWahCx",
        "colab_type": "text"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6XzV6-3RTGe",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import pandas as pd, numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "import numpy as np\n",
        "print('TF version',tf.__version__)\n",
        "\n",
        "\n",
        "use_cuda = True ##If True, GPU will be used"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P2bY1g66abD",
        "colab_type": "text"
      },
      "source": [
        "Download Roberta-Based and Roberta Tensorflow from https://huggingface.co/roberta-base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Hmz0RX6J85",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Cv0d3v6qo4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPBTI365dJLo",
        "colab_type": "text"
      },
      "source": [
        "### Mount Your Own Gdrive\n",
        "\n",
        "Below command will require you to validate your account, and it will provide you with a temporary access code to paste in the required field"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT-bkv7WRK7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%ls /gdrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7T-lNV39yOJ",
        "colab_type": "text"
      },
      "source": [
        "### Load the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B94gEgYzgV72",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Before running below command, make sure you have...\n",
        "- Created a *'tweet-sentiment-extraction'* folder inside the *'Colab Notebooks'* directory\n",
        "- Uploaded the *train.csv* and *test.csv* files to the *'tweet-sentiment-extraction'* folder \n",
        "\n",
        "Finally, make sure you have a folder called *'models'* inside the *'tweet-sentiment-extraction'* directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I1aa6YfTGJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/gdrive/My Drive/Colab Notebooks/tweet-sentiment-extraction/train.csv')\n",
        "test = pd.read_csv('/gdrive/My Drive/Colab Notebooks/tweet-sentiment-extraction/test.csv')\n",
        "\n",
        "\n",
        "\n",
        "#sub_df = pd.read_csv('/gdrive/My Drive/Colab Notebooks/tweet-sentiment-extraction/sample_submission.csv') #Optional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho6qQLOmzy7-",
        "colab_type": "text"
      },
      "source": [
        "### Prepare the Dat\n",
        "\n",
        "Set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEEY7JDVBRrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 128\n",
        "\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "            vocab_file = 'Your-Path/vocab.json',\n",
        "            merges_file = 'Your-Path/merges.txt',\n",
        "            lowercase =True,\n",
        "            add_prefix_space=True\n",
        ")\n",
        "###Use the tokenizer to encode the postive,negative,neutral\n",
        "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoWxi6_y8cQp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Define jaccard score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmnE_Tgqlcc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2):\n",
        "    a = set(str1.lower().split())\n",
        "    b = set(str2.lower().split())\n",
        "    if (len(a)==0) & (len(b)==0): return 0.5\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkfYSwUz8nfl",
        "colab_type": "text"
      },
      "source": [
        "Cutomize loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8FMKmbQF7au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits = False, label_smoothing = 0.20)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmf3wMvo9e6p",
        "colab_type": "text"
      },
      "source": [
        "In here we are going to create our manual creating with a fixed maximum length:\n",
        "\n",
        "* input ids \n",
        "* attention mask\n",
        "* token type id mask \n",
        "* start tokens - tells where the answer start in the context\n",
        "* end tokens -tells where the answer end in the context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXWKO9NUDJso",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1gtVov3ms4iFp9TAOYvUU1dbygDqkyPj2)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXEgpSz18u_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = train.shape[0]\n",
        "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(train.shape[0]):\n",
        "    \n",
        "    # FIND OVERLAP\n",
        "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
        "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
        "    idx = text1.find(text2)\n",
        "    chars = np.zeros((len(text1)))\n",
        "    chars[idx:idx+len(text2)]=1\n",
        "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "    enc = tokenizer.encode(text1) \n",
        "        \n",
        "    # ID_OFFSETS\n",
        "    offsets = []; idx=0\n",
        "    for t in enc.ids:\n",
        "        w = tokenizer.decode([t])\n",
        "        offsets.append((idx,idx+len(w)))\n",
        "        idx += len(w)\n",
        "    \n",
        "    # START END TOKENS\n",
        "    toks = []\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.sum(chars[a:b])\n",
        "        if sm>0: toks.append(i) \n",
        "        \n",
        "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
        "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask[k,:len(enc)+5] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+1] = 1\n",
        "        end_tokens[k,toks[-1]+1] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rI5KNGg_ryS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = test.shape[0]\n",
        "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(test.shape[0]):\n",
        "        \n",
        "    # INPUT_IDS\n",
        "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "    enc = tokenizer.encode(text1)                \n",
        "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
        "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask_t[k,:len(enc.ids)+5] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFB0LNoZ9C-1",
        "colab_type": "text"
      },
      "source": [
        "### Build our own tensorflow model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi7kKnNFGm2c",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1b8_j2f-Jdnau1XroacvMMpyGBtADPGbU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBRBDPcT7oTQ",
        "colab_type": "text"
      },
      "source": [
        "In here we are going build a customized structure in following layers:\n",
        "\n",
        "*   Roberta Model Layer\n",
        "*   Drop out layer\n",
        "*   Conv1D layer\n",
        "*   LeakyRelu Layer\n",
        "*   Fully Connected Dense Layer\n",
        "*   Soft Max Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9Fdz0yyBX6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import RobertaConfig,TFRobertaModel\n",
        "import pickle\n",
        "\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    config_path = RobertaConfig.from_pretrained('Your-Path/config-roberta-base.json')\n",
        "    roberta_model = TFRobertaModel.from_pretrained('Your-Path/pretrained-roberta-base.h5', config=config_path)\n",
        "    x = roberta_model(ids, attention_mask = att, token_type_ids=tok)\n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu8C-zphp9DP",
        "colab_type": "text"
      },
      "source": [
        "# SECTION 2: Train a tensorflow model with K-fold and predict the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWyefDO8HwLS",
        "colab_type": "text"
      },
      "source": [
        "You will have your training set,validation set, and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDEYgbSKpC9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
        "\n",
        "    print('#'*25)\n",
        "    print('### FOLD %i'%(fold+1))\n",
        "    print('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "        \n",
        "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "        \n",
        "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
        "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
        "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
        "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
        "    \n",
        "    print('Loading model...')\n",
        "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
        "    \n",
        "    print('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    print('Predicting Test...')\n",
        "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "    preds_start += preds[0]/skf.n_splits\n",
        "    preds_end += preds[1]/skf.n_splits\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "        if a>b: \n",
        "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-1:b])\n",
        "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
        "    jac.append(np.mean(all))\n",
        "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
        "    print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5h3q91zA2cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpkfQo6zIZAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all = []\n",
        "for k in range(input_ids_t.shape[0]):\n",
        "    a = np.argmax(preds_start[k,])\n",
        "    b = np.argmax(preds_end[k,])\n",
        "    if a>b: \n",
        "        st = test.loc[k,'text']\n",
        "    else:\n",
        "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
        "        enc = tokenizer.encode(text1)\n",
        "        st = tokenizer.decode(enc.ids[a-1:b])\n",
        "    all.append(st)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiaEIIxQIe2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test['selected_text'] = all\n",
        "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
        "pd.set_option('max_colwidth', 60)\n",
        "test.sample(25)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}